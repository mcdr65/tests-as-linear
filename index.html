<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Common statistical tests are linear models (or: how to teach stats)</title>

<script src="index_files/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="index_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="index_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="index_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="index_files/navigation-1.1/tabsets.js"></script>
<link href="index_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="index_files/highlightjs-9.12.0/highlight.js"></script>
<script src="index_files/htmlwidgets-1.3/htmlwidgets.js"></script>
<link href="index_files/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="index_files/datatables-binding-0.6/datatables.js"></script>
<link href="index_files/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="index_files/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="index_files/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
<link href="index_files/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="index_files/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@jonaslindeloev">
<meta name="twitter:title" content="Common statistical tests are linear models (or: how to teach stats)">
<meta name="twitter:image" content="https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.png">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1026978-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  
  gtag('config', 'UA-1026978-2');
</script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Common statistical tests are linear models (or: how to teach stats)</h1>

</div>


<p><link rel="stylesheet" type="text/css" href="include/style.css"></p>
<!-- From https://stackoverflow.com/a/37839683/1297830 -->
<link rel="stylesheet" type="text/css" href="include/hideOutput.css">
<script src="include/hideOutput.js"></script>
<p>By Jonas Kristoffer Lindeløv (<a href="https://lindeloev.net">blog</a>, <a href="http://personprofil.aau.dk/117060">profile</a>).<br /> Last updated: 28 June, 2019 (See <a href="https://github.com/lindeloev/tests-as-linear/commits/master">changelog</a>).<br /> Check out the <a href="https://eigenfoo.xyz/tests-as-linear/">Python version</a> and the <a href="https://twitter.com/jonaslindeloev/status/1110907133833502721">Twitter summary</a>.</p>
<!-- Social sharing. From simplesharebuttons.com -->
<style type="text/css">
  #share-buttons img {
    width: 40px;
    padding-right: 15px;
    border: 0;
    box-shadow: 0;
    display: inline;
    vertical-align: top;
  }
</style>
<div id="share-buttons">
<!-- Twitter -->
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><a href="https://twitter.com/intent/tweet?text=Common%20statistical%20tests%20are%20linear%20models%20(or:%20how%20to%20teach%20stats)%20https%3A%2F%2Flindeloev.github.io%2Ftests-as-linear%20via%20%40jonaslindeloev" class="twitter-hashtag-button" data-size="large" data-related="jonaslindeloev" data-show-count="false">Share on Twitter</a>    <!-- Facebook --><a href="http://www.facebook.com/sharer.php?u=https://lindeloev.github.io/tests-as-linear/" target="_blank"><img src="https://simplesharebuttons.com/images/somacro/facebook.png" alt="Facebook" /></a><!-- LinkedIn --><a href="http://www.linkedin.com/shareArticle?mini=true&amp;url=https://lindeloev.github.io/tests-as-linear/" target="_blank"><img src="https://simplesharebuttons.com/images/somacro/linkedin.png" alt="LinkedIn" /></a><!-- Digg --><a href="http://www.digg.com/submit?url=https://lindeloev.github.io/tests-as-linear/" target="_blank"><img src="https://simplesharebuttons.com/images/somacro/diggit.png" alt="Digg" /></a><!-- Reddit --><a href="http://reddit.com/submit?url=https://lindeloev.github.io/tests-as-linear/&amp;title=Common statistical tests are linear models (or: how to teach stats)" target="_blank"><img src="https://simplesharebuttons.com/images/somacro/reddit.png" alt="Reddit" /></a><!-- Email --><a href="mailto:?Subject=Common statistical tests are linear models (or: how to teach stats)&amp;Body=https://lindeloev.github.io/tests-as-linear/"><img src="https://simplesharebuttons.com/images/somacro/email.png" alt="Email" /></a></p>
</div>
<p><br /></p>
<p>This document is summarised in the table below. It shows the linear models underlying common parametric and “non-parametric” tests. Formulating all the tests in the same language highlights the many similarities between them. Get it <a href="linear_tests_cheat_sheet.png">as an image</a> or <a href="linear_tests_cheat_sheet.pdf">as a PDF</a>.</p>
<hr />
<p><a href="linear_tests_cheat_sheet.pdf"><img src="linear_tests_cheat_sheet.png" /></a></p>
<hr />
<div id="the-simplicity-underlying-common-tests" class="section level1">
<h1><span class="header-section-number">1</span> The simplicity underlying common tests</h1>
<p>Most of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This beautiful simplicity means that there is less to learn. In particular, it all comes down to <span class="math inline">\(y = a \cdot x + b\)</span> which most students know from highschool. Unfortunately, stats intro courses are usually taught as if each test is an independent tool, needlessly making life more complicated for students and teachers alike.</p>
<p>This needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model.</p>
<p>For this reason, I think that teaching linear models first and foremost and <em>then</em> name-dropping the special cases along the way makes for an excellent teaching strategy, emphasizing <em>understanding</em> over rote learning. Since linear models are the same across frequentist, Bayesian, and permutation-based inferences, I’d argue that it’s better to start with modeling than p-values, type-1 errors, Bayes factors, or other inferences.</p>
<p>Concerning the teaching of <em>“non-parametric”</em> tests in intro-courses, I think that we can justify <a href="https://en.wikipedia.org/wiki/Lie-to-children">lying-to-children</a> and teach “non-parametric”" tests as if they are merely ranked versions of the corresponding parametric tests. It is much better for students to think “ranks!” than to believe that you can magically throw away assumptions. Indeed, the Bayesian equivalents of “non-parametric”" tests implemented in <a href="https://jasp-stats.org">JASP</a> <a href="https://arxiv.org/abs/1712.06941">literally just do (latent) ranking</a> and that’s it. For the frequentist “non-parametric”" tests considered here, this approach is highly accurate for N &gt; 15.</p>
<center>
<img src="https://www.picsellmedia.com/wp-content/uploads/2017/01/shutterstock_336913772.jpg" />
</center>
<p><br /></p>
<p>Use the menu to jump to your favourite section. There are links to lots of similar (though more scattered) stuff under <a href="#links">sources</a> and <a href="#course">teaching materials</a>. I hope that you will join in suggesting improvements or submitting improvements yourself in <a href="https://github.com/lindeloev/tests-as-linear">the Github repo to this page</a>. Let’s make it awesome!</p>
</div>
<div id="settings-and-toy-data" class="section level1">
<h1><span class="header-section-number">2</span> Settings and toy data</h1>
Unfold this if you want to see functions and other settings for this notebook:
<div class="fold s">
<pre class="r"><code># Load packages for data handling and plotting
library(tidyverse)
library(patchwork)
library(broom)

# Reproducible &quot;random&quot; results
set.seed(40)

# Generate normal data with known parameters
rnorm_fixed = function(N, mu = 0, sd = 1)
  scale(rnorm(N)) * sd + mu

# Plot style.
theme_axis = function(P,
                      jitter = FALSE,
                      xlim = c(-0.5, 2),
                      ylim = c(-0.5, 2),
                      legend.position = NULL) {
  P = P + theme_bw(15) +
    geom_segment(
      x = -1000, xend = 1000,
      y = 0, yend = 0,
      lty = 2, color = &#39;dark gray&#39;, lwd = 0.5
    ) +
    geom_segment(
      x = 0, xend = 0,
      y = -1000, yend = 1000,
      lty = 2, color = &#39;dark gray&#39;, lwd = 0.5
    ) +
    coord_cartesian(xlim = xlim, ylim = ylim) +
    theme(
      axis.title = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      legend.position = legend.position
    )
  
  # Return jittered or non-jittered plot?
  if (jitter) {
    P + geom_jitter(width = 0.1, size = 2)
  }
  else {
    P + geom_point(size = 2)
  }
}</code></pre>
</div>
<p>For a start, we’ll keep it simple and play with three standard normals in wide (<code>a</code>, <code>b</code>, <code>c</code>) and long format (<code>value</code>, <code>group</code>):</p>
<pre class="r"><code># Wide format (sort of)
#y = rnorm_fixed(50, mu=0.3, sd=2)  # Almost zero mean.
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c(&#39;y1&#39;, &#39;y2&#39;), each = 50)</code></pre>
</div>
<div id="correlation" class="section level1">
<h1><span class="header-section-number">3</span> Pearson and Spearman correlation</h1>
<div id="theory-as-linear-models" class="section level3">
<h3><span class="header-section-number">3.0.1</span> Theory: As linear models</h3>
<p>Model: the recipe for <span class="math inline">\(y\)</span> is a slope (<span class="math inline">\(\beta_1\)</span>) times <span class="math inline">\(x\)</span> plus an intercept (<span class="math inline">\(\beta_0\)</span>, aka a straight line).</p>
<p><span class="math inline">\(y = \beta_0 + \beta_1 x \qquad \mathcal{H}_0: \beta_1 = 0\)</span></p>
<p>… which is a math-y way of writing the good old <span class="math inline">\(y = ax + b\)</span> (here ordered as <span class="math inline">\(y = b + ax\)</span>). In R we are lazy and write <code>y ~ 1 + x</code> which R reads like <code>y = 1*number + x*othernumber</code> and the task of t-tests, lm, etc., is simply to find the numbers that best predict <span class="math inline">\(y\)</span>.</p>
<p>Either way you write it, it’s an intercept (<span class="math inline">\(\beta_0\)</span>) and a slope (<span class="math inline">\(\beta_1\)</span>) yielding a straight line:</p>
<div class="fold s">
<pre class="r"><code># Fixed correlation
D_correlation = data.frame(MASS::mvrnorm(30, mu = c(0.9, 0.9), Sigma = matrix(c(1, 0.8, 1, 0.8), ncol = 2), empirical = TRUE))  # Correlated data

# Add labels (for next plot)
D_correlation$label_num = sprintf(&#39;(%.1f,%.1f)&#39;, D_correlation$X1, D_correlation$X2)
D_correlation$label_rank = sprintf(&#39;(%i,%i)&#39;, rank(D_correlation$X1), rank(D_correlation$X2))

# Plot it
fit = lm(I(X2 * 0.5 + 0.4) ~ I(X1 * 0.5 + 0.2), D_correlation)
intercept_pearson = coefficients(fit)[1]

P_pearson = ggplot(D_correlation, aes(x=X1*0.5+0.2, y=X2*0.5+0.4)) +
  geom_smooth(method=lm, se=FALSE, lwd=2, aes(colour=&#39;beta_1&#39;)) + 
  geom_segment(x=-100, xend=100, 
               y=intercept_pearson, yend=intercept_pearson, 
               lwd=2, aes(color=&quot;beta_0&quot;)) + 
  scale_color_manual(name=NULL, values=c(&quot;blue&quot;, &quot;red&quot;), labels=c(bquote(beta[0]*&quot; (intercept)&quot;), bquote(beta[1]*&quot; (slope)&quot;)))
  
theme_axis(P_pearson, legend.position = c(0.4, 0.9))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<p>This is often simply called a <strong>regression</strong> model which can be extended to <strong>multiple regression</strong> where there are several <span class="math inline">\(\beta\)</span>s and on the right-hand side multiplied with the predictors. Everything below, from <a href="#t1">one-sample t-test</a> to <a href="#anova2">two-way ANOVA</a> are just special cases of this system. Nothing more, nothing less.</p>
<p>As the name implies, the <strong>Spearman rank correlation</strong> is a <strong>Pearson correlation</strong> on rank-transformed <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<p><span class="math inline">\(rank(y) = \beta_0 + \beta_1 \cdot rank(x) \qquad \mathcal{H}_0: \beta_1 = 0\)</span></p>
<p>I’ll introduce <a href="#rank">ranks</a> in a minute. For now, notice that the correlation coefficient of the linear model is identical to a “real” Pearson correlation, but p-values are an approximation which is is <a href="simulations/simulate_spearman.html">appropriate for samples greater than N=10 and almost perfect when N &gt; 20</a>.</p>
<p>Such a nice and non-mysterious equivalence that many students are left unaware of! Visualizing them side by side including data labels, we see this rank-transformation in action:</p>
<div class="fold s">
<pre class="r"><code># Spearman intercept
intercept_spearman = coefficients(lm(rank(X2) ~ rank(X1), D_correlation))[1]

# Spearman plot
P_spearman = ggplot(D_correlation, aes(x=rank(X1), y=rank(X2))) +
  geom_smooth(method=lm, se=FALSE, lwd=2, aes(color=&#39;beta_1&#39;)) + 
  geom_text(aes(label=label_rank), nudge_y=1, size=3, color=&#39;dark gray&#39;) + 
  geom_segment(x=-100, xend=100, 
               y=intercept_spearman, yend=intercept_spearman, 
               lwd=2, aes(color=&#39;beta_0&#39;)) + 
  scale_color_manual(name=NULL, values=c(&quot;blue&quot;, &quot;red&quot;), labels=c(bquote(beta[0]*&quot; (intercept)&quot;), bquote(beta[1]*&quot; (slope)&quot;)))

# Stich together using patchwork
(theme_axis(P_pearson, legend.position=c(0.5, 0.1)) + geom_text(aes(label=label_num), nudge_y=0.1, size=3, color=&#39;dark gray&#39;) + labs(title=&#39;         Pearson&#39;)) + (theme_axis(P_spearman, xlim=c(-7.5, 30), ylim=c(-7.5, 30), legend.position=c(0.5, 0.1)) + labs(title=&#39;         Spearman&#39;))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="rank" class="section level3">
<h3><span class="header-section-number">3.0.2</span> Theory: rank-transformation</h3>
<p><code>rank</code> simply takes a list of numbers and “replace” them with the integers of their rank (1st smallest, 2nd smallest, 3rd smallest, etc.). So the result of the rank-transformation <code>rank(c(3.6, 3.4, -5.0, 8.2))</code> is <code>3, 2, 1, 4</code>. See that in the figure above?</p>
<p>A <em>signed</em> rank is the same, just where we rank according to absolute size first and then add in the sign second. So the signed rank here would be <code>2, 1, -3, 4</code>. Or in code:</p>
<pre class="r"><code>signed_rank = function(x) sign(x) * rank(abs(x))</code></pre>
<p>I hope I don’t offend anyone when I say that ranks are easy; yet it’s all you need to do to convert most parametric tests into their “non-parametric” counterparts! One interesting implication is that <em>many “non-parametric tests” are about as parametric as their parametric counterparts with means, standard deviations, homogeneity of variance, etc. - just on rank-transformed data</em>. That’s why I put “non-parametric” in quotation marks.</p>
</div>
<div id="r-code-pearson-correlation" class="section level3">
<h3><span class="header-section-number">3.0.3</span> R code: Pearson correlation</h3>
<p>It couldn’t be much simpler to run these models in R. They yield identical <code>p</code> and <code>t</code>, but there’s a catch: <code>lm</code> gives you the <em>slope</em> and even though that is usually much more interpretable and informative than the <em>correlation coefficient</em> <em>r</em>, you may still want <em>r</em>. Luckily, the slope becomes <code>r</code> if <code>x</code> and <code>y</code> have identical standard deviations. For now, we will use <code>scale(x)</code> to make <span class="math inline">\(SD(x) = 1.0\)</span> and <span class="math inline">\(SD(y) = 1.0\)</span>:</p>
<pre class="r"><code>a = cor.test(y, x, method = &quot;pearson&quot;) # Built-in
b = lm(y ~ 1 + x) # Equivalent linear model: y = Beta0*1 + Beta1*x
c = lm(scale(y) ~ 1 + scale(x))  # On scaled vars to recover r</code></pre>
Results: <div id="htmlwidget-526ecd6fc2f0b7381ee6" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-526ecd6fc2f0b7381ee6">{"x":{"filter":"none","data":[["cor.test","lm scaled","lm"],[0.738,0.738,0.738],[-0.3365,-0.3365,-0.3365],[-0.0485,-0.0485,-0.0872],[-0.3225,-0.3384,-0.6081],[0.233,0.2414,0.4337]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>model<\/th>\n      <th>p.value<\/th>\n      <th>t<\/th>\n      <th>r<\/th>\n      <th>conf.low<\/th>\n      <th>conf.high<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"searching":false,"lengthChange":false,"ordering":false,"autoWidth":true,"bPaginate":false,"bInfo":false,"paging":false,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5]}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<div class="fold o">
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  y and x
## t = -0.33651, df = 48, p-value = 0.738
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.3225066  0.2329799
## sample estimates:
##         cor 
## -0.04851394 
## 
## 
## Call:
## lm(formula = y ~ 1 + x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.6265 -1.1753 -0.3718  0.6607  5.7109 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.09522    0.25647  -0.371    0.712
## x           -0.08718    0.25907  -0.337    0.738
## 
## Residual standard error: 1.814 on 48 degrees of freedom
## Multiple R-squared:  0.002354,   Adjusted R-squared:  -0.01843 
## F-statistic: 0.1132 on 1 and 48 DF,  p-value: 0.738
## 
## 
## Call:
## lm(formula = scale(y) ~ 1 + scale(x))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.4616 -0.6541 -0.2069  0.3677  3.1780 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  1.722e-17  1.427e-01   0.000    1.000
## scale(x)    -4.851e-02  1.442e-01  -0.337    0.738
## 
## Residual standard error: 1.009 on 48 degrees of freedom
## Multiple R-squared:  0.002354,   Adjusted R-squared:  -0.01843 
## F-statistic: 0.1132 on 1 and 48 DF,  p-value: 0.738</code></pre>
</div>
<p>The CIs are not exactly identical, but very close.</p>
</div>
<div id="r-code-spearman-correlation" class="section level3">
<h3><span class="header-section-number">3.0.4</span> R code: Spearman correlation</h3>
<p>Note that we can interpret the slope which is the number of ranks <span class="math inline">\(y\)</span> change for each rank on <span class="math inline">\(x\)</span>. I think that this is a pretty interesting number. However, the intercept is less interpretable since it lies at <span class="math inline">\(rank(x) = 0\)</span> which is impossible since x starts at 1.</p>
<p>See the identical <code>r</code> (now “rho”) and <code>p</code>:</p>
<pre class="r"><code># Spearman correlation
a = cor.test(y, x, method = &quot;spearman&quot;) # Built-in
b = lm(rank(y) ~ 1 + rank(x)) # Equivalent linear model</code></pre>
Let’s look at the results: <div id="htmlwidget-0ed9471d24578b65b4fa" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-0ed9471d24578b65b4fa">{"x":{"filter":"none","data":[["cor.test","lm"],[0.7072,0.708],[-0.0543,-0.0543]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>model<\/th>\n      <th>p.value<\/th>\n      <th>rho<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"searching":false,"lengthChange":false,"ordering":false,"autoWidth":true,"bPaginate":false,"bInfo":false,"paging":false,"columnDefs":[{"className":"dt-right","targets":[1,2]}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<div class="fold o">
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  y and x
## S = 21956, p-value = 0.7072
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##         rho 
## -0.05430972 
## 
## 
## Call:
## lm(formula = rank(y) ~ 1 + rank(x))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -23.8211 -12.0056  -0.0272  12.5215  25.6677 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 26.88490    4.22287   6.366 6.89e-08 ***
## rank(x)     -0.05431    0.14412  -0.377    0.708    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.71 on 48 degrees of freedom
## Multiple R-squared:  0.00295,    Adjusted R-squared:  -0.01782 
## F-statistic: 0.142 on 1 and 48 DF,  p-value: 0.708</code></pre>
</div>
</div>
</div>
<div id="one-mean" class="section level1">
<h1><span class="header-section-number">4</span> One mean</h1>
<div id="t1" class="section level2">
<h2><span class="header-section-number">4.1</span> One sample t-test and Wilcoxon signed-rank</h2>
<div id="theory-as-linear-models-1" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Theory: As linear models</h3>
<p><strong>t-test</strong> model: A single number predicts <span class="math inline">\(y\)</span>.</p>
<p><span class="math inline">\(y = \beta_0 \qquad \mathcal{H}_0: \beta_0 = 0\)</span></p>
<p>In other words, it’s our good old <span class="math inline">\(y = \beta_0 + \beta_1*x\)</span> where the last term is gone since there is no <span class="math inline">\(x\)</span> (essentially <span class="math inline">\(x=0\)</span>, see left figure below).</p>
<p>The same is to a very close approximately true for <strong>Wilcoxon signed-rank test</strong>, just with the <a href="#rank">signed ranks</a> of <span class="math inline">\(y\)</span> instead of <span class="math inline">\(y\)</span> itself (see right panel below).</p>
<p><span class="math inline">\(signed\_rank(y) = \beta_0\)</span></p>
<p><a href="simulations/simulate_wilcoxon.html">This approximation is good enough when the sample size is larger than 14 and almost perfect if the sample size is larger than 50</a>.</p>
<div class="fold s">
<pre class="r"><code># T-test
D_t1 = data.frame(y = rnorm_fixed(20, 0.5, 0.6),
                  x = runif(20, 0.93, 1.07))  # Fix mean and SD

P_t1 = ggplot(D_t1, aes(y = y, x = 0)) + 
  stat_summary(fun.y=mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y.., color=&#39;beta_0&#39;), lwd=2) +
  scale_color_manual(name = NULL, values = c(&quot;blue&quot;), labels = c(bquote(beta[0] * &quot; (intercept)&quot;))) +
  
  geom_text(aes(label = round(y, 1)), nudge_x = 0.2, size = 3, color = &#39;dark gray&#39;) + 
  labs(title=&#39;         T-test&#39;)

# Wilcoxon
D_t1_rank = data.frame(y = signed_rank(D_t1$y))

P_t1_rank = ggplot(D_t1_rank, aes(y = y, x = 0)) + 
  stat_summary(fun.y = mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y..,  color = &#39;beta_0&#39;), lwd = 2) +
  scale_color_manual(name = NULL, values = c(&quot;blue&quot;), labels = c(bquote(beta[0] * &quot; (intercept)&quot;))) +

  geom_text(aes(label = y), nudge_x = 0.2, size = 3, color = &#39;dark gray&#39;) + 
  labs(title=&#39;         Wilcoxon&#39;)


# Stich together using patchwork
theme_axis(P_t1, ylim = c(-1, 2), legend.position = c(0.6, 0.1)) + 
  theme_axis(P_t1_rank, ylim = NULL,  legend.position = c(0.6, 0.1))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="r-code-one-sample-t-test" class="section level3">
<h3><span class="header-section-number">4.1.2</span> R code: One-sample t-test</h3>
<p>Try running the R code below and see that the linear model (<code>lm</code>) produces the same <span class="math inline">\(t\)</span>, <span class="math inline">\(p\)</span>, and <span class="math inline">\(r\)</span> as the built-in <code>t.test</code>. The confidence interval is not presented in the output of <code>lm</code> but is also identical if you use <code>confint(lm(...))</code>:</p>
<pre class="r"><code># Built-in t-test
a = t.test(y)

